"""
ä¸»ç¨‹åºå…¥å£ - SimCSEç‰ˆæœ¬
è¿è¡Œæ–¹å¼: python main.py

SimCSE-ABSA: SimCSE-based Cross-lingual ABSA
æ ¸å¿ƒæ”¹è¿›ï¼š
1. SimCSE (Dropout-based Unsupervised Contrastive Learning)
2. æ— éœ€ç¿»è¯‘æ•°æ®ï¼Œé¿å…ç¿»è¯‘å™ªå£°
3. Attention Pooling æå–æƒ…æ„Ÿç‰¹å¾
"""
import torch
from torch.utils.data import DataLoader
from transformers import XLMRobertaTokenizer
import warnings

warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from config import Config
from utils import set_seed, save_config, print_model_info, get_timestamp
from data_preprocessing import DataPreprocessor
from dataset import CrossLingualDataset
from model import XLMSentimentModel
from trainer import Trainer


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("SimCSE-ABSA: SimCSE-based Cross-lingual ABSA")
    print("è·¨è¯­è¨€æƒ…æ„Ÿåˆ†æé¡¹ç›® - è‹±è¯­ -> æ—¥è¯­æƒ…æ„Ÿè¿ç§»")
    print("=" * 70)
    print("\næ ¸å¿ƒæ”¹è¿›:")
    print("  [1] SimCSE - Dropout-based Contrastive Learning")
    print("  [2] æ— éœ€ç¿»è¯‘æ•°æ® - é¿å…ç¿»è¯‘å™ªå£°")
    print("  [3] Attention Pooling - æ•æ‰æƒ…æ„Ÿå…³é”®è¯")
    print("=" * 70)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    config = Config()
    config.create_dirs()

    # è®¾ç½®éšæœºç§å­
    set_seed(config.SEED)
    print(f"\néšæœºç§å­: {config.SEED}")
    print(f"ä½¿ç”¨è®¾å¤‡: {config.DEVICE}")

    # æ‰“å°é…ç½®ä¿¡æ¯
    config.print_config()

    # æ‰“å°æ€§èƒ½ä¼˜åŒ–å»ºè®®
    tips = config.get_performance_tips()
    if tips:
        print("=" * 70)
        print("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        print("=" * 70)
        for tip in tips:
            print(f"  {tip}")
        print("=" * 70 + "\n")

    # ä¿å­˜é…ç½®
    timestamp = get_timestamp()
    config_save_path = f"{config.LOG_DIR}/config_{timestamp}.json"
    save_config(config, config_save_path)

    # ========== æ­¥éª¤1: æ•°æ®é¢„å¤„ç† ==========
    print("\n" + "=" * 70)
    print("æ­¥éª¤1: æ•°æ®é¢„å¤„ç† (SimCSEæ¨¡å¼)")
    print("=" * 70)

    preprocessor = DataPreprocessor(config)
    train_data, valid_data, test_data = preprocessor.prepare_datasets()

    # ========== æ­¥éª¤2: åŠ è½½Tokenizer ==========
    print("=" * 70)
    print("æ­¥éª¤2: åŠ è½½Tokenizer")
    print("=" * 70)

    tokenizer = XLMRobertaTokenizer.from_pretrained(config.MODEL_PATH)
    print("âœ“ TokenizeråŠ è½½å®Œæˆ\n")

    # ========== æ­¥éª¤3: åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ ==========
    print("=" * 70)
    print("æ­¥éª¤3: åˆ›å»ºæ•°æ®åŠ è½½å™¨")
    print("=" * 70)

    # è®­ç»ƒé›†
    train_dataset = CrossLingualDataset(
        train_data,
        tokenizer,
        config.MAX_LENGTH
    )

    # DataLoaderé…ç½®
    dataloader_kwargs = {
        'batch_size': config.BATCH_SIZE,
        'num_workers': config.NUM_WORKERS,
        'pin_memory': config.PIN_MEMORY and config.DEVICE.type == 'cuda',
    }

    if config.NUM_WORKERS > 0 and config.PERSISTENT_WORKERS:
        dataloader_kwargs['persistent_workers'] = True

    train_loader = DataLoader(
        train_dataset,
        shuffle=True,
        **dataloader_kwargs
    )

    # éªŒè¯é›†
    valid_dataset = CrossLingualDataset(
        valid_data,
        tokenizer,
        config.MAX_LENGTH
    )
    valid_loader = DataLoader(
        valid_dataset,
        shuffle=False,
        **dataloader_kwargs
    )

    # æµ‹è¯•é›†ï¼ˆæ—¥è¯­ï¼‰
    test_dataset = CrossLingualDataset(
        test_data,
        tokenizer,
        config.MAX_LENGTH
    )
    test_loader = DataLoader(
        test_dataset,
        shuffle=False,
        **dataloader_kwargs
    )

    print(f"âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"âœ“ éªŒè¯æ‰¹æ¬¡æ•°: {len(valid_loader)}")
    print(f"âœ“ æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
    print(f"âœ“ DataLoaderé…ç½®: workers={config.NUM_WORKERS}, "
          f"pin_memory={config.PIN_MEMORY and config.DEVICE.type == 'cuda'}")
    print()

    # ========== æ­¥éª¤4: åˆå§‹åŒ–SimCSE-ABSAæ¨¡å‹ ==========
    print("=" * 70)
    print("æ­¥éª¤4: åˆå§‹åŒ– SimCSE-ABSA æ¨¡å‹")
    print("=" * 70)

    model = XLMSentimentModel(
        model_path=config.MODEL_PATH,
        num_classes=config.NUM_CLASSES,
        projection_dim=config.PROJECTION_DIM,
        dropout_rate=config.DROPOUT_RATE
    ).to(config.DEVICE)

    print_model_info(model)

    # ========== æ­¥éª¤5: è®­ç»ƒ ==========
    print("=" * 70)
    print("æ­¥éª¤5: å¼€å§‹è®­ç»ƒ")
    print("=" * 70)

    trainer = Trainer(
        model,
        train_loader,
        valid_loader,
        test_loader,
        config
    )
    trainer.train()

    print("\n" + "=" * 70)
    print("SimCSE-ABSA é¡¹ç›®è¿è¡Œå®Œæˆï¼")
    print("=" * 70)
    print(f"\næ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®: {config.CHECKPOINT_DIR}")
    print(f"æ—¥å¿—ä¿å­˜ä½ç½®: {config.LOG_DIR}")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
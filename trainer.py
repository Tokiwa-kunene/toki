"""
è®­ç»ƒå™¨æ¨¡å— - SimCSEç‰ˆæœ¬
æ ¸å¿ƒå˜æ›´ï¼š
1. å®ç° SimCSE (åŒä¸€è¾“å…¥ä¸¤æ¬¡forwardï¼Œåˆ©ç”¨Dropout)
2. ä¼˜åŒ– Checkpoint ä¿å­˜ç­–ç•¥ï¼ˆé˜²æ­¢ç£ç›˜çˆ†æ»¡ï¼‰
3. è¿½è¸ªæ‰€æœ‰å†å²æœ€ä½³æŒ‡æ ‡ï¼ˆéªŒè¯é›†F1ã€æµ‹è¯•é›†Accã€æµ‹è¯•é›†F1ï¼‰
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
import os
import time
from datetime import timedelta


class Trainer:
    """SimCSE-ABSA è®­ç»ƒå™¨"""

    def __init__(self, model, train_loader, valid_loader, test_loader, config):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            model: SimCSE-ABSA æ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            config: é…ç½®å¯¹è±¡
        """
        self.model = model
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.test_loader = test_loader
        self.config = config

        # ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(train_loader) * config.NUM_EPOCHS
        warmup_steps = int(config.WARMUP_RATIO * total_steps)
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

        # æŸå¤±å‡½æ•°
        self.cls_criterion = nn.CrossEntropyLoss()
        self.temperature = config.TEMPERATURE
        self.alpha = config.ALPHA

        print(f"\næŸå¤±æƒé‡é…ç½®:")
        print(f"  Alpha = {self.alpha}")
        print(f"  åˆ†ç±»æŸå¤±æƒé‡ = {1 - self.alpha:.3f}")
        print(f"  SimCSEæŸå¤±æƒé‡ = {self.alpha:.3f}")

        # æœ€ä½³æŒ‡æ ‡è¿½è¸ªï¼ˆå®Œæ•´ç‰ˆï¼‰
        self.best_valid_f1 = 0.0  # æœ€ä½³éªŒè¯é›†F1
        self.best_valid_f1_test_acc = 0.0  # æœ€ä½³éªŒè¯F1æ—¶çš„æµ‹è¯•å‡†ç¡®ç‡
        self.best_valid_f1_test_f1 = 0.0  # æœ€ä½³éªŒè¯F1æ—¶çš„æµ‹è¯•F1

        # æ–°å¢ï¼šè¿½è¸ªæµ‹è¯•é›†æœ¬èº«çš„å†å²æœ€é«˜åˆ†
        self.best_test_acc = 0.0  # æµ‹è¯•é›†å†å²æœ€é«˜å‡†ç¡®ç‡
        self.best_test_f1 = 0.0  # æµ‹è¯•é›†å†å²æœ€é«˜F1
        self.best_test_acc_epoch = 0  # è¾¾åˆ°æœ€é«˜å‡†ç¡®ç‡çš„epoch
        self.best_test_f1_epoch = 0  # è¾¾åˆ°æœ€é«˜F1çš„epoch

        # æ—¶é—´ç»Ÿè®¡
        self.epoch_times = []
        self.total_start_time = None

    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ªepochï¼ˆSimCSEç‰ˆæœ¬ï¼‰

        æ ¸å¿ƒï¼šå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œä¸¤æ¬¡forwardï¼Œåˆ©ç”¨Dropoutç”Ÿæˆæ­£æ ·æœ¬å¯¹

        Args:
            epoch: å½“å‰epochç¼–å·

        Returns:
            (avg_loss, avg_cls_loss, avg_cl_loss, epoch_time)
        """
        self.model.train()  # ç¡®ä¿ Dropout å¼€å¯ï¼
        total_loss = 0
        total_cls_loss = 0
        total_cl_loss = 0

        epoch_start_time = time.time()

        progress_bar = tqdm(
            self.train_loader,
            desc=f"è®­ç»ƒ Epoch {epoch + 1}/{self.config.NUM_EPOCHS}"
        )

        for batch_idx, batch in enumerate(progress_bar):
            input_ids = batch['input_ids'].to(self.config.DEVICE)
            attention_mask = batch['attention_mask'].to(self.config.DEVICE)
            labels = batch['label'].to(self.config.DEVICE)

            # ===== SimCSE æ ¸å¿ƒï¼šåŒä¸€è¾“å…¥ä¸¤æ¬¡ forward =====
            # ç¬¬ä¸€æ¬¡ forwardï¼ˆDropout çŠ¶æ€1ï¼‰
            logits_1, features_1 = self.model(input_ids, attention_mask)

            # ç¬¬äºŒæ¬¡ forwardï¼ˆDropout çŠ¶æ€2ï¼Œä¸åŒçš„éšæœºæ©ç ï¼‰
            logits_2, features_2 = self.model(input_ids, attention_mask)

            # ===== è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆä½¿ç”¨ç¬¬ä¸€æ¬¡çš„logitsï¼‰ =====
            loss_cls = self.cls_criterion(logits_1, labels)

            # ===== è®¡ç®— SimCSE å¯¹æ¯”æŸå¤± =====
            # features_1 å’Œ features_2 äº’ä¸ºæ­£æ ·æœ¬å¯¹
            loss_cl = self._compute_simcse_loss(features_1, features_2)

            # ===== å½’ä¸€åŒ–æ€»æŸå¤± =====
            loss = (1.0 - self.alpha) * loss_cls + self.alpha * loss_cl

            # ===== åå‘ä¼ æ’­ =====
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                max_norm=self.config.MAX_GRAD_NORM
            )
            self.optimizer.step()
            self.scheduler.step()

            # è®°å½•æŸå¤±
            total_loss += loss.item()
            total_cls_loss += loss_cls.item()
            total_cl_loss += loss_cl.item()

            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'cls': f'{loss_cls.item():.4f}',
                'simcse': f'{loss_cl.item():.4f}'
            })

        epoch_time = time.time() - epoch_start_time

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(self.train_loader)
        avg_cls_loss = total_cls_loss / len(self.train_loader)
        avg_cl_loss = total_cl_loss / len(self.train_loader)

        return avg_loss, avg_cls_loss, avg_cl_loss, epoch_time

    def _compute_simcse_loss(self, z1, z2):
        """
        è®¡ç®— SimCSE å¯¹æ¯”æŸå¤±ï¼ˆInfoNCEï¼‰

        å¯¹äºåŒä¸€ä¸ªbatchï¼š
        - æ­£æ ·æœ¬ï¼š(z1[i], z2[i]) - åŒä¸€ä¸ªè¾“å…¥çš„ä¸¤æ¬¡forward
        - è´Ÿæ ·æœ¬ï¼šbatchå†…çš„å…¶ä»–æ ·æœ¬

        Args:
            z1: ç¬¬ä¸€æ¬¡forwardçš„ç‰¹å¾ [batch_size, projection_dim]
            z2: ç¬¬äºŒæ¬¡forwardçš„ç‰¹å¾ [batch_size, projection_dim]

        Returns:
            loss: SimCSE å¯¹æ¯”æŸå¤±
        """
        batch_size = z1.size(0)
        device = z1.device

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # sim[i,j] = cosine_similarity(z1[i], z2[j]) / temperature
        sim_matrix = torch.matmul(z1, z2.T) / self.temperature

        # å¯¹è§’çº¿å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹ï¼ˆz1[i] å’Œ z2[i] æ¥è‡ªåŒä¸€è¾“å…¥ï¼‰
        labels = torch.arange(batch_size, device=device)

        # InfoNCE æŸå¤±ï¼šè®©å¯¹è§’çº¿å…ƒç´ çš„ç›¸ä¼¼åº¦æœ€å¤§
        loss = F.cross_entropy(sim_matrix, labels)

        return loss

    def evaluate(self, data_loader, dataset_name="éªŒè¯é›†"):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            (accuracy, f1): å‡†ç¡®ç‡å’ŒF1åˆ†æ•°
        """
        self.model.eval()
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch in tqdm(data_loader, desc=f"è¯„ä¼°{dataset_name}", leave=False):
                input_ids = batch['input_ids'].to(self.config.DEVICE)
                attention_mask = batch['attention_mask'].to(self.config.DEVICE)
                labels = batch['label'].to(self.config.DEVICE)

                # åªéœ€è¦ä¸€æ¬¡forwardï¼ˆè¯„ä¼°æ—¶ä¸éœ€è¦Dropoutå¤šæ ·æ€§ï¼‰
                logits, _ = self.model(input_ids, attention_mask)
                preds = torch.argmax(logits, dim=1)

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        accuracy = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average='binary')

        return accuracy, f1

    def save_checkpoint(self, epoch, valid_acc, valid_f1, test_acc, test_f1, is_best=False, is_last=False):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆä¼˜åŒ–çš„ä¿å­˜ç­–ç•¥ï¼‰

        Args:
            epoch: å½“å‰epoch
            valid_acc: éªŒè¯é›†å‡†ç¡®ç‡
            valid_f1: éªŒè¯é›†F1
            test_acc: æµ‹è¯•é›†å‡†ç¡®ç‡
            test_f1: æµ‹è¯•é›†F1
            is_best: æ˜¯å¦ä¸ºæœ€ä½³éªŒè¯F1æ¨¡å‹
            is_last: æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªepoch
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'valid_accuracy': valid_acc,
            'valid_f1': valid_f1,
            'test_accuracy': test_acc,
            'test_f1': test_f1,
            'best_valid_f1': self.best_valid_f1,
            'best_test_acc': self.best_test_acc,
            'best_test_f1': self.best_test_f1,
            'alpha': self.alpha,
            'temperature': self.temperature
        }

        # åªä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆé˜²æ­¢ç£ç›˜çˆ†æ»¡ï¼‰
        if is_best:
            best_path = os.path.join(
                self.config.CHECKPOINT_DIR,
                'best_model.pt'
            )
            torch.save(checkpoint, best_path)
            print(f"  âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")

        # å¯é€‰ï¼šä¿å­˜æœ€åä¸€ä¸ªepoch
        if is_last and self.config.SAVE_LAST_EPOCH:
            last_path = os.path.join(
                self.config.CHECKPOINT_DIR,
                'last_model.pt'
            )
            torch.save(checkpoint, last_path)
            print(f"  âœ“ æœ€åæ¨¡å‹å·²ä¿å­˜: {last_path}")

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 70)
        print("å¼€å§‹ SimCSE-ABSA è®­ç»ƒ")
        print("=" * 70)
        print(f"SimCSE ç­–ç•¥: Dropout-based Unsupervised Contrastive Learning")
        print(f"æŸå¤±æƒé‡é…ç½® (Alpha={self.alpha}):")
        print(f"  åˆ†ç±»æŸå¤±: {(1 - self.alpha) * 100:.1f}%")
        print(f"  SimCSEæŸå¤±: {self.alpha * 100:.1f}%")
        print("=" * 70)

        # å¼€å§‹è®¡æ—¶
        self.total_start_time = time.time()

        for epoch in range(self.config.NUM_EPOCHS):
            print(f"\n{'=' * 70}")
            print(f"Epoch {epoch + 1}/{self.config.NUM_EPOCHS}")
            print('=' * 70)

            # è®­ç»ƒä¸€ä¸ªepoch
            avg_loss, avg_cls_loss, avg_cl_loss, epoch_time = self.train_epoch(epoch)
            self.epoch_times.append(epoch_time)

            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            print("\nåœ¨éªŒè¯é›†(è‹±è¯­)ä¸Šè¯„ä¼°...")
            eval_start = time.time()
            valid_acc, valid_f1 = self.evaluate(self.valid_loader, "éªŒè¯é›†")

            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            print("åœ¨æµ‹è¯•é›†(æ—¥è¯­)ä¸Šè¯„ä¼°...")
            test_acc, test_f1 = self.evaluate(self.test_loader, "æµ‹è¯•é›†")
            eval_time = time.time() - eval_start

            # ===== æ›´æ–°æ‰€æœ‰å†å²æœ€ä½³æŒ‡æ ‡ =====
            # 1. æ£€æŸ¥éªŒè¯é›†F1æ˜¯å¦ä¸ºæœ€ä½³
            is_best_valid = valid_f1 > self.best_valid_f1
            if is_best_valid:
                self.best_valid_f1 = valid_f1
                self.best_valid_f1_test_acc = test_acc
                self.best_valid_f1_test_f1 = test_f1

            # 2. æ›´æ–°æµ‹è¯•é›†å†å²æœ€é«˜å‡†ç¡®ç‡
            if test_acc > self.best_test_acc:
                self.best_test_acc = test_acc
                self.best_test_acc_epoch = epoch + 1

            # 3. æ›´æ–°æµ‹è¯•é›†å†å²æœ€é«˜F1
            if test_f1 > self.best_test_f1:
                self.best_test_f1 = test_f1
                self.best_test_f1_epoch = epoch + 1

            # æ—¶é—´ç»Ÿè®¡
            elapsed_time = time.time() - self.total_start_time
            avg_epoch_time = sum(self.epoch_times) / len(self.epoch_times)
            remaining_epochs = self.config.NUM_EPOCHS - (epoch + 1)
            estimated_remaining = remaining_epochs * (avg_epoch_time + eval_time)

            # æ‰“å°ç»“æœ
            print(f"\n{'=' * 70}")
            print(f"Epoch {epoch + 1} ç»“æœ")
            print('=' * 70)
            print(f"è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
            print(f"  åˆ†ç±»æŸå¤±: {avg_cls_loss:.4f} (æƒé‡: {1 - self.alpha:.3f})")
            print(f"  SimCSEæŸå¤±: {avg_cl_loss:.4f} (æƒé‡: {self.alpha:.3f})")
            print(f"\néªŒè¯é›†(è‹±è¯­):")
            print(f"  å‡†ç¡®ç‡: {valid_acc:.4f}")
            print(f"  F1åˆ†æ•°: {valid_f1:.4f}")
            print(f"\næµ‹è¯•é›†(æ—¥è¯­ - Zero-shot):")
            print(f"  å‡†ç¡®ç‡: {test_acc:.4f}")
            print(f"  F1åˆ†æ•°: {test_f1:.4f}")

            # æ ‡è®°æ–°è®°å½•
            markers = []
            if is_best_valid:
                markers.append("ğŸ† æ–°çš„æœ€ä½³éªŒè¯F1")
            if test_acc == self.best_test_acc:
                markers.append("â­ æµ‹è¯•é›†å†å²æœ€é«˜å‡†ç¡®ç‡")
            if test_f1 == self.best_test_f1:
                markers.append("â­ æµ‹è¯•é›†å†å²æœ€é«˜F1")

            if markers:
                print(f"\n" + " | ".join(markers))

            # æ—¶é—´ä¿¡æ¯
            print(f"\nâ±ï¸  æ—¶é—´ç»Ÿè®¡:")
            print(f"  æœ¬è½®è®­ç»ƒ: {timedelta(seconds=int(epoch_time))}")
            print(f"  æœ¬è½®è¯„ä¼°: {timedelta(seconds=int(eval_time))}")
            print(f"  å·²ç”¨æ—¶é—´: {timedelta(seconds=int(elapsed_time))}")
            print(f"  é¢„è®¡å‰©ä½™: {timedelta(seconds=int(estimated_remaining))}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_last_epoch = (epoch + 1) == self.config.NUM_EPOCHS
            if is_best_valid or is_last_epoch:
                print()
                self.save_checkpoint(
                    epoch, valid_acc, valid_f1,
                    test_acc, test_f1,
                    is_best=is_best_valid,
                    is_last=is_last_epoch
                )

            print('=' * 70)

        # æ€»æ—¶é—´ç»Ÿè®¡
        total_time = time.time() - self.total_start_time

        # ===== å®Œæ•´çš„è®­ç»ƒæ€»ç»“æŠ¥å‘Š =====
        print("\n" + "=" * 70)
        print("SimCSE-ABSA è®­ç»ƒå®Œæˆï¼")
        print("=" * 70)

        print(f"\nğŸ“Š å®Œæ•´è®­ç»ƒæŠ¥å‘Š:")
        print("=" * 70)

        # 1. éªŒè¯é›†æœ€ä½³F1åŠå…¶å¯¹åº”çš„æµ‹è¯•é›†æ€§èƒ½
        print(f"\n[1] åŸºäºéªŒè¯é›†F1é€‰æ‹©çš„æœ€ä½³æ¨¡å‹:")
        print(f"    æœ€ä½³éªŒè¯F1: {self.best_valid_f1:.4f}")
        print(f"    å¯¹åº”æµ‹è¯•å‡†ç¡®ç‡: {self.best_valid_f1_test_acc:.4f}")
        print(f"    å¯¹åº”æµ‹è¯•F1: {self.best_valid_f1_test_f1:.4f}")

        # 2. æµ‹è¯•é›†å†å²æœ€é«˜å‡†ç¡®ç‡
        print(f"\n[2] æµ‹è¯•é›†å†å²æœ€é«˜å‡†ç¡®ç‡:")
        print(f"    Best Test Accuracy: {self.best_test_acc:.4f}")
        print(f"    è¾¾åˆ°äº Epoch {self.best_test_acc_epoch}")

        # 3. æµ‹è¯•é›†å†å²æœ€é«˜F1
        print(f"\n[3] æµ‹è¯•é›†å†å²æœ€é«˜F1:")
        print(f"    Best Test F1: {self.best_test_f1:.4f}")
        print(f"    è¾¾åˆ°äº Epoch {self.best_test_f1_epoch}")

        # 4. æ—¶é—´ç»Ÿè®¡
        print(f"\nâ±ï¸  è®­ç»ƒæ—¶é—´ç»Ÿè®¡:")
        print(f"    æ€»è®­ç»ƒæ—¶é—´: {timedelta(seconds=int(total_time))}")
        print(f"    å¹³å‡æ¯è½®: {timedelta(seconds=int(total_time / self.config.NUM_EPOCHS))}")

        print("=" * 70)

        # æç¤ºä¿¡æ¯
        print(f"\nğŸ’¡ è¯´æ˜:")
        print(f"  - [1] æ˜¯ä¿å­˜çš„ best_model.pt çš„æ€§èƒ½")
        print(f"  - [2][3] æ˜¯æµ‹è¯•é›†åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¾¾åˆ°çš„å³°å€¼")
        print(f"  - å¦‚æœ [2][3] æ˜æ˜¾é«˜äº [1]ï¼Œè¯´æ˜å­˜åœ¨è¿‡æ‹Ÿåˆ")
        print("=" * 70 + "\n")